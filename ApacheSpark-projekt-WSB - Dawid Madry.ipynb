{"cells": [{"cell_type": "markdown", "id": "861b6d4a-0de6-42ba-97a5-beef1f82f292", "metadata": {}, "source": "# Projekt Apache Spark"}, {"cell_type": "markdown", "id": "7b301ae8-ceff-4dbf-8d04-75bb4eb52480", "metadata": {}, "source": "# Wprowadzenie\n\nWykorzystuj\u0105c ten notatnik jako szablon zrealizuj projekt Apache Spark zgodnie z przydzielonym zestawem. \n\nKilka uwag:\n\n* Nie modyfikuj ani nie usuwaj paragraf\u00f3w *markdown* w tym notatniku, chyba \u017ce wynika to jednoznacznie z instrukcji. \n* Istniej\u0105ce paragrafy zawieraj\u0105ce *kod* uzupe\u0142nij w razie potrzeby zgodnie z instrukcjami\n    - nie usuwaj ich\n    - nie usuwaj zawartych w nich instrukcji oraz kodu\n    - nie modyfikuj ich, je\u015bli instrukcje jawnie tego nie nakazuj\u0105\n* Mo\u017cesz dodawa\u0107 nowe paragrafy zar\u00f3wno zawieraj\u0105ce kod jak i komentarze dotycz\u0105ce tego kodu (markdown)"}, {"cell_type": "markdown", "id": "e69d12f1-1013-4c74-b6aa-686ccfcbdd5c", "metadata": {}, "source": "# Tre\u015b\u0107 projektu\n\nPoni\u017cej w paragrafie markdown wstaw tytu\u0142 przydzielonego zestawu"}, {"cell_type": "markdown", "id": "adfc4ff6-4d43-49ed-a0d1-8b6988eaec16", "metadata": {}, "source": "# Zestaw 6 \u2013 fifa_players"}, {"cell_type": "markdown", "id": "14408872-4060-4a18-ba98-e1f60637d182", "metadata": {}, "source": "# Dzia\u0142ania wst\u0119pne \n\nUtworzenie kontekst\u00f3w"}, {"cell_type": "code", "execution_count": 12, "id": "c9e5bb04-67c7-4886-af9e-70fc5d334c36", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/12/10 18:27:15 INFO SparkEnv: Registering MapOutputTracker\n24/12/10 18:27:16 INFO SparkEnv: Registering BlockManagerMaster\n24/12/10 18:27:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/10 18:27:16 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n\n# Spark session & context\nspark = SparkSession.builder.getOrCreate()"}, {"cell_type": "code", "execution_count": 13, "id": "a08580f5-a5d2-41bc-88ae-892ee11297c0", "metadata": {"tags": []}, "outputs": [], "source": "sc = spark.sparkContext"}, {"cell_type": "markdown", "id": "5d987ef2-c630-4552-9c96-099414ab7e7c", "metadata": {}, "source": "W poni\u017cszym paragrafie uzupe\u0142nij polecenia definiuj\u0105ce poszczeg\u00f3lne zmienne. \n\nPami\u0119taj aby\u015b:\n\n* w p\u00f3\u017aniejszym kodzie, dla wszystkich cze\u015bci projektu, korzysta\u0142 z tych zdefiniowanych zmiennych. Wykorzystuj je analogicznie jak parametry\n* przed ostateczn\u0105 rejestracj\u0105 projektu usun\u0105\u0142 ich warto\u015bci, tak aby nie pozostawia\u0107 w notatniku niczego co mog\u0142oby identyfikowa\u0107 Ciebie jako jego autora"}, {"cell_type": "code", "execution_count": 5, "id": "ad23aede-2679-4a99-a7be-a672f217490b", "metadata": {"tags": []}, "outputs": [], "source": "# pe\u0142na \u015bcie\u017cka do katalogu w zasobniku zawieraj\u0105cego podkatalogi `datasource1` i `datasource4` \n# z danymi \u017ar\u00f3d\u0142owymi\ninput_dir = \"/home/dawiden1/data/zestaw6\""}, {"cell_type": "markdown", "id": "a17c06f9-e311-45e5-8206-e003882d1606", "metadata": {}, "source": "Nie modyfikuj poni\u017cszych paragraf\u00f3w. Wykonaj je i u\u017cywaj zdefniowanych poni\u017cej zmiennych jak parametr\u00f3w Twojego programu."}, {"cell_type": "code", "execution_count": 6, "id": "d06be3d4-cb35-48f9-8356-519ce31fbede", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n# \u015bcie\u017cki dla danych \u017ar\u00f3d\u0142owych \ndatasource1_dir = input_dir + \"/datasource1/*\"\ndatasource4_dir = input_dir + \"/datasource4/*\"\n\n# nazwy i \u015bcie\u017cki dla wynik\u00f3w dla misji g\u0142\u00f3wnej \n# cz\u0119\u015b\u0107 1 (Spark Core - RDD) \nrdd_result_dir = \"/tmp/output1\"\n\n# cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame)\ndf_result_table = \"output2\""}, {"cell_type": "code", "execution_count": 7, "id": "12c588e6-528e-4c80-9344-962e4eb3d9f3", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nimport os\ndef remove_file(file):\n    if os.path.exists(file):\n        os.remove(file)\n\nremove_file(\"metric_functions.py\")\nremove_file(\"tools_functions.py\")"}, {"cell_type": "code", "execution_count": 8, "id": "82343036-3e4e-4a16-80c1-6ab8e88d4600", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "3322"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "# NIE ZMIENIA\u0106\nimport requests\nr = requests.get(\"https://jankiewicz.pl/bigdata/metric_functions.py\", allow_redirects=True)\nopen('metric_functions.py', 'wb').write(r.content)\nr = requests.get(\"https://jankiewicz.pl/bigdata/tools_functions.py\", allow_redirects=True)\nopen('tools_functions.py', 'wb').write(r.content)"}, {"cell_type": "code", "execution_count": 9, "id": "63b37efc-37f1-4b13-af59-e75b65da7b24", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n%run metric_functions.py\n%run tools_functions.py"}, {"cell_type": "markdown", "id": "cc5355ad-a6dd-4f1a-a25d-36692679e885", "metadata": {}, "source": "Poni\u017csze paragrafy maj\u0105 na celu usun\u0105\u0107 ewentualne pozosta\u0142o\u015bci poprzednich uruchomie\u0144 tego lub innych notatnik\u00f3w"}, {"cell_type": "code", "execution_count": 14, "id": "7fb73a60-dcbd-4888-8135-f4ad5f763889", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Deleted /tmp/output1\nSuccessfully deleted file in HDFS: /tmp/output1\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 1 (Spark Core - RDD) \ndelete_dir(spark, rdd_result_dir)"}, {"cell_type": "code", "execution_count": 16, "id": "f13a8362-c647-4dff-9b39-bdfdc92ce9d9", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "The table output2 does not exist.\nError deleting file file:/spark-warehouse/output2: Command '['hadoop', 'fs', '-rm', '-r', 'file:/spark-warehouse/output2']' returned non-zero exit status 1.\n"}, {"name": "stderr", "output_type": "stream", "text": "rm: `file:/spark-warehouse/output2': No such file or directory\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame) \ndrop_table(spark, df_result_table)"}, {"cell_type": "markdown", "id": "1a0317e5-2279-4979-8578-7e2c96ac3573", "metadata": {}, "source": "***Uwaga!***\n\nUruchom poni\u017cszy paragraf i sprawd\u017a czy adres pod kt\u00f3rym dost\u0119pny Apache Spark Application UI jest poprawny wywo\u0142uj\u0105c nast\u0119pny testowy paragraf. \n\nW razie potrzeby okre\u015bl samodzielnie poprawny adres pod kt\u00f3rym dost\u0119pny Apache Spark Application UI"}, {"cell_type": "code", "execution_count": 29, "id": "b8dc6f54-b0ec-4bf0-ab30-845c985f69fb", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "'http://big-data-cluster-m.europe-west4-a.c.braided-grammar-436313-g5.internal:41409'"}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": "# adres URL, pod kt\u00f3rym dost\u0119pny Apache Spark Application UI (REST API)\n# \nspark_ui_address = extract_host_and_port(spark, \"http://localhost:4040\")\nspark_ui_address"}, {"cell_type": "code", "execution_count": 30, "id": "9e8a4c61-13fc-488c-8e0d-67e69bb92903", "metadata": {"tags": []}, "outputs": [], "source": "# testowy paragraf\ntest_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "b94ffcfb-fd09-4bbe-ad41-de27b862f46d", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 1 - Spark Core (RDD)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "8af00c41-02a9-4a85-b3c6-bc41098edbe2", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "1b370100-3b56-4668-bb26-3db00b8ad04b", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "ef66612b-909e-412d-9148-4cf588774ff9", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 22, "id": "5fecf1b2-237a-4dc4-9cc7-adc2e806fa4f", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "3bae0494-3a28-4d7e-95f0-b6ca7eee9f8f", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a rozwi\u0105zanie *misji g\u0142\u00f3wnej* oparte na *RDD API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *RDD API* tego wymaga. \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 47, "id": "63155041-12fd-4657-89e9-b1b212afc545", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "data = sc.textFile(datasource1_dir)\nparsed_data = data.map(lambda line: line.split(\";\"))\n\n# Utworzenie klucza (league_id, club_team_id) i warto\u015bci 1 dla ka\u017cdego gracza\nplayers_per_team = parsed_data.map(lambda row: ((row[16], row[17]), 1))\n\n# Zliczenie liczby graczy w ka\u017cdej dru\u017cynie\nteam_player_counts = players_per_team.reduceByKey(lambda x, y: x + y)\n\n# Filtracja dru\u017cyn, kt\u00f3re maj\u0105 co najmniej 11 graczy\nteams_with_enough_players = team_player_counts.filter(lambda x: x[1] >= 11)\n\n# Utworzenie klucza league_id i warto\u015bci 1 dla ka\u017cdej dru\u017cyny\nteams_per_league = teams_with_enough_players.map(lambda x: (x[0][0], 1))\n\n# Zliczenie liczby dru\u017cyn w ka\u017cdej lidze\nleague_team_counts = teams_per_league.reduceByKey(lambda x, y: x + y)\n\n# Filtracja lig, kt\u00f3re maj\u0105 co najmniej 10 dru\u017cyn\nleagues_with_enough_teams = league_team_counts.filter(lambda x: x[1] >= 10)\n\n# Pobranie identyfikator\u00f3w lig z co najmniej 10 dru\u017cynami\nvalid_league_ids = leagues_with_enough_teams.map(lambda x: x[0]).collect()\n\n# Filtracja pi\u0142karzy graj\u0105cych w ligach spe\u0142niaj\u0105cych wymagania\nfiltered_players = parsed_data.filter(lambda row: row[16] in valid_league_ids)\n"}, {"cell_type": "code", "execution_count": 57, "id": "7cf537dc-6fd7-4dab-8027-0edb5f4af0b0", "metadata": {"tags": []}, "outputs": [], "source": "#Narodowosc\nplayers_by_nationality = filtered_players.map(lambda row: (row[25], row)).groupByKey()\nnationality_stats = players_by_nationality.mapValues(lambda players: (\n    sum(player[10] for player in players),  # sum_value_eur\n    sum(player[11] for player in players) / len(players),  # avg_wage_eur\n    sum(int(player[12]) for player in players) / len(players),  # avg_age\n    len(players)  # count_players\n))\n\n# Klub\nplayers_by_club = filtered_players.map(lambda row: (row[18], row)).groupByKey()\nclub_stats = players_by_club.mapValues(lambda players: (\n    sum(player[10] for player in players),  # sum_value_eur\n    sum(player[11] for player in players) / len(players),  # avg_wage_eur\n    sum(int(player[12]) for player in players) / len(players),  # avg_age\n    len(players)  # count_players\n))\n\n# Liga\nplayers_by_league = filtered_players.map(lambda row: (row[16], row)).groupByKey()\nleague_stats = players_by_league.mapValues(lambda players: (\n    sum(player[10] for player in players),  # sum_value_eur\n    sum(player[11] for player in players) / len(players),  # avg_wage_eur\n    sum(int(player[12]) for player in players) / len(players),  # avg_age\n    len(players)  # count_players\n))\n\n# \u0141\u0105czenie\nall_stats = nationality_stats.union(club_stats).union(league_stats)\n\n# Tworzymy RDD z kategori\u0105 (nationality, club, league) oraz odpowiednimi statystykami\nresult = all_stats.map(lambda x: (x[0], {\n    'category': 'nationality' if x[0] in nationality_stats else 'club' if x[0] in club_stats else 'league',\n    'name': x[0],\n    'sum_value_eur': x[1][0],\n    'avg_wage_eur': x[1][1],\n    'avg_age': x[1][2],\n    'count_players': x[1][3]\n}))\n"}, {"cell_type": "code", "execution_count": 60, "id": "ceb7d53c-e271-48fb-8357-811a40143c01", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/10 19:23:15 WARN TaskSetManager: Lost task 11.0 in stage 60.0 (TID 818) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 16): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\n24/12/10 19:23:17 ERROR TaskSetManager: Task 11 in stage 60.0 failed 4 times; aborting job\n24/12/10 19:23:17 ERROR SparkHadoopWriter: Aborting job job_202412101922428486702530165726204_0327.\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 60.0 failed 4 times, most recent failure: Lost task 11.3 in stage 60.0 (TID 829) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 16): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2457) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2478) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2510) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1632) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [scala-library-2.12.18.jar:?]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) [spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) [spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410) [spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1632) [spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:579) [spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:578) [spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45) [spark-core_2.12-3.5.1.jar:3.5.1]\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]\n\tat py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]\n\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) ~[scala-library-2.12.18.jar:?]\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) ~[scala-library-2.12.18.jar:?]\n\tat scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n24/12/10 19:23:17 WARN TaskSetManager: Lost task 25.2 in stage 60.0 (TID 831) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 17): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 11 in stage 60.0 failed 4 times, most recent failure: Lost task 11.3 in stage 60.0 (TID 829) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 16): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:)\n24/12/10 19:23:17 WARN TaskSetManager: Lost task 1.2 in stage 60.0 (TID 830) (big-data-cluster-w-1.europe-west4-a.c.braided-grammar-436313-g5.internal executor 15): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 11 in stage 60.0 failed 4 times, most recent failure: Lost task 11.3 in stage 60.0 (TID 829) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 16): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:)\n24/12/10 19:23:17 WARN TaskSetManager: Lost task 0.2 in stage 60.0 (TID 832) (big-data-cluster-w-1.europe-west4-a.c.braided-grammar-436313-g5.internal executor 18): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 11 in stage 60.0 failed 4 times, most recent failure: Lost task 11.3 in stage 60.0 (TID 829) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 16): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:)\n"}, {"ename": "Py4JJavaError", "evalue": "An error occurred while calling o1557.saveAsObjectFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1632)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1632)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:578)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 60.0 failed 4 times, most recent failure: Lost task 11.3 in stage 60.0 (TID 829) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 16): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2457)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2478)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2510)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 39 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_stats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsPickleFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd_result_dir\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py:3342\u001b[0m, in \u001b[0;36mRDD.saveAsPickleFile\u001b[0;34m(self, path, batchSize)\u001b[0m\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3341\u001b[0m     ser \u001b[38;5;241m=\u001b[39m BatchedSerializer(CPickleSerializer(), batchSize)\n\u001b[0;32m-> 3342\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mser\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsObjectFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1557.saveAsObjectFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1632)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1632)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:578)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 60.0 failed 4 times, most recent failure: Lost task 11.3 in stage 60.0 (TID 829) (big-data-cluster-w-0.europe-west4-a.c.braided-grammar-436313-g5.internal executor 16): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2457)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2478)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2510)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 39 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"/tmp/ipykernel_11444/2687952428.py\", line 4, in <lambda>\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1930)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"]}], "source": "all_stats.saveAsPickleFile(rdd_result_dir)"}, {"cell_type": "markdown", "id": "f0911dc9-e636-4ac0-b962-5d9dc595679f", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": null, "id": "ce40be31-e7c6-47d8-8712-1a58f542500c", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "11fa58fe-cea7-4e62-940d-1169b2e1cad5", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": null, "id": "eca6e627-0ce5-4c48-b441-3bcc14e32f36", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "0927c1aa-ea14-4052-99cc-8c7ad311087e", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "2f9c85b6-5111-49d9-8b48-c2008347949c", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 31, "id": "6ab8c395-e13e-4de1-8380-a6e19bb25cbd", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "4c875c26-f0f9-42bb-b06a-50e0f9df44ae", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a rozwi\u0105zanie *misji g\u0142\u00f3wnej* swojego projektu oparte o *DataFrame API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *DataFrame API* nie jest w stanie wszystkiego \"naprawi\u0107\". \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 32, "id": "0c54269d-1ebc-44d4-9a38-65f5a5e490dd", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import functions as f\n\nspark = SparkSession.builder \\\n    .appName(\"Spark Project\") \\\n    .config(\"spark.hadoop.hadoop.home.dir\", \"C:/Hadoop\") \\\n    .getOrCreate()\n\nspark.conf.set(\"spark.sql.debug.maxToStringFields\", 10000)\n\ndatasource1 = spark.read.csv(\n    datasource1_dir,\n    sep=\";\",  # separator\n    header=False,  # nag\u0142\u00f3wki\n    inferSchema=True  # wykrycie typ\u00f3w kolumn\n)\n\ndatasource4 = spark.read.csv(\n    datasource4_dir,\n    sep=\";\",  # separator\n    header=False,  # nag\u0142\u00f3wki\n    inferSchema=True  # wykrycie typ\u00f3w kolumn\n)\n\nleagues_columns = (datasource4.select(f.col(\"_c0\").alias(\"league_id\"),\n                                      f.col(\"_c1\").alias(\"league_name\"),\n                                      f.col(\"_c2\").alias(\"league_level\")))\n\nselected_columns = (datasource1.select(f.col(\"_c0\").alias(\"player_id\"),\n                                       f.col(\"_c7\").alias(\"player_positions\"),\n                                       f.col(\"_c10\").alias(\"value_eur\"),\n                                       f.col(\"_c11\").alias(\"wage_eur\"),\n                                       f.col(\"_c12\").alias(\"age\"),\n                                       f.col(\"_c16\").alias(\"league_id\"),\n                                       f.col(\"_c17\").alias(\"club_team_id\"),\n                                       f.col(\"_c18\").alias(\"club_name\"),\n                                       f.col(\"_c25\").alias(\"nationality_name\"))\n                    )"}, {"cell_type": "code", "execution_count": 33, "id": "06dba7be-b39e-4b13-9913-3a4cae465e75", "metadata": {"tags": []}, "outputs": [], "source": "agg_players = (selected_columns.groupBy(\"league_id\", \"club_team_id\")\n               .agg(f.count(\"player_id\").alias(\"total_players\"))\n               .where(f.col(\"total_players\") >= 11)\n               )\n\nagg_leagues = (agg_players.groupBy(\"league_id\")\n               .agg(f.count(\"club_team_id\").alias(\"total_clubs\"))\n               .where(f.col(\"total_clubs\") >= 10)\n               )\n\nreduced_data = (selected_columns\n                .join(agg_leagues, on=\"league_id\", how=\"inner\")\n                )\n\ntop_clubs = (reduced_data.groupBy(\"club_name\")\n             .agg(f.sum(\"value_eur\").alias(\"sum_value_eur\"),\n                    f.avg(\"wage_eur\").alias(\"avg_wage_eur\"),\n                    f.avg(\"age\").alias(\"avg_age\"),\n                    f.count(\"player_id\").alias(\"count_players\"),\n                    f.collect_set(\"player_positions\").alias(\"player_positions\"))\n                .orderBy(f.desc(\"avg_wage_eur\"))\n                .select(\n    f.lit(\"club\").alias(\"category\"),\n    f.col(\"club_name\").alias(\"name\"),\n    f.format_number(f.col(\"sum_value_eur\"), 0).alias(\"sum_value_eur\"),\n    f.round(f.col(\"avg_wage_eur\"), 3).alias(\"avg_wage_eur\"),\n    f.round(f.col(\"avg_age\"), 0).cast(\"int\").alias(\"avg_age\"),\n    f.col(\"count_players\").alias(\"count_players\"),\n    f.array_distinct(f.col(\"player_positions\")).alias(\"player_positions\"))\n                .limit(3))\n\ntop_leagues = (reduced_data.join(leagues_columns, 'league_id')\n               .groupBy(\"league_name\")\n               .agg(f.sum(\"value_eur\").alias(\"sum_value_eur\"),\n                    f.avg(\"wage_eur\").alias(\"avg_wage_eur\"),\n                    f.avg(\"age\").alias(\"avg_age\"),\n                    f.count(\"player_id\").alias(\"count_players\"),\n                    f.collect_set(\"player_positions\").alias(\"player_positions\"))\n               .orderBy(f.desc(\"avg_wage_eur\"))\n               .select(\n    f.lit(\"league\").alias(\"category\"),\n    f.col(\"league_name\").alias(\"name\"),\n    f.format_number(f.col(\"sum_value_eur\"), 0).alias(\"sum_value_eur\"),\n    f.round(f.col(\"avg_wage_eur\"), 3).alias(\"avg_wage_eur\"),\n    f.round(f.col(\"avg_age\"), 0).cast(\"int\").alias(\"avg_age\"),\n    f.col(\"count_players\").alias(\"count_players\"),\n    f.array_distinct(f.col(\"player_positions\")).alias(\"player_positions\"))\n               .limit(3))\n\ntop_nationalities = (reduced_data.groupBy(\"nationality_name\")\n               .agg(f.sum(\"value_eur\").alias(\"sum_value_eur\"),\n                    f.avg(\"wage_eur\").alias(\"avg_wage_eur\"),\n                    f.avg(\"age\").alias(\"avg_age\"),\n                    f.count(\"player_id\").alias(\"count_players\"),\n                    f.collect_set(\"player_positions\").alias(\"player_positions\"))\n               .orderBy(f.desc(\"avg_wage_eur\"))\n               .select(\n    f.lit(\"nationality\").alias(\"category\"),\n    f.col(\"nationality_name\").alias(\"name\"),\n    f.format_number(f.col(\"sum_value_eur\"), 0).alias(\"sum_value_eur\"),\n    f.round(f.col(\"avg_wage_eur\"), 3).alias(\"avg_wage_eur\"),\n    f.round(f.col(\"avg_age\"), 0).cast(\"int\").alias(\"avg_age\"),\n    f.col(\"count_players\").alias(\"count_players\"),\n    f.col(\"player_positions\")).alias(\"player_positions\")\n                .limit(3))"}, {"cell_type": "code", "execution_count": 34, "id": "947b90f8-69c4-469d-9d34-d86cae5ea7b1", "metadata": {"tags": []}, "outputs": [], "source": "fifa_players = top_clubs.union(top_leagues).union(top_nationalities)\nfifa_players.createOrReplaceTempView(df_result_table)"}, {"cell_type": "markdown", "id": "44ae8d1c-0739-47e0-aa9e-cd4db027590b", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 35, "id": "ff10e8f9-b751-472f-ab65-f5a58532d053", "metadata": {"tags": []}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "e8f1cdce-fbf0-4b96-bc6d-471c5b2baa42", "metadata": {}, "source": "# Analiza wynik\u00f3w i wydajno\u015bci *misji g\u0142\u00f3wnych*"}, {"cell_type": "markdown", "id": "c4e2e990-5973-4c2e-b821-c70854c299b9", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 1 - Spark Core (RDD)"}, {"cell_type": "code", "execution_count": null, "id": "dcba5ea6-e55c-4b08-8ded-c495bee1dd84", "metadata": {}, "outputs": [], "source": "# Wczytanie wynik\u00f3w z pliku pickle\nword_counts = sc.pickleFile(rdd_result_dir)\n\n# Wy\u015bwietlenie 50 pierwszych element\u00f3w\nresult_sample = word_counts.take(50)\nfor item in result_sample:\n    print(item)"}, {"cell_type": "code", "execution_count": null, "id": "ad5bc0f2-35ee-4e57-82ca-3300f4fc478e", "metadata": {}, "outputs": [], "source": "subtract_metrics(after_rdd_metrics, before_rdd_metrics)"}, {"cell_type": "markdown", "id": "efc730f1-4b5e-4a68-8a86-11768918fcf4", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)"}, {"cell_type": "code", "execution_count": 23, "id": "1af1f027-880d-4e4a-ac5e-106919540d86", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 33:======================================>                   (2 + 1) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------------------+--------------+------------+-------+-------------+--------------------+\n|   category|                name| sum_value_eur|avg_wage_eur|avg_age|count_players|    player_positions|\n+-----------+--------------------+--------------+------------+-------+-------------+--------------------+\n|       club|         Real Madrid| 1,050,650,000|  118934.211|     24|           38|[CM, CAM, CM, ST,...|\n|       club|     Manchester City| 1,281,910,000|  101384.615|     25|           39|[ST, RW, LW, RW, ...|\n|       club|        FC Barcelona| 1,131,875,000|   91571.429|     24|           49|[RW, ST, CAM, CAM...|\n|     league|      Premier League|10,481,385,000|   24216.743|     24|         1959|[CB, LB, CDM, RM,...|\n|     league|             La Liga| 7,612,290,000|   21643.132|     25|         1143|[LM, RM, LW, CAM,...|\n|     league|             Serie A| 7,729,900,000|   19339.702|     26|         1544|[RW, ST, CF, CF, ...|\n|nationality|United Arab Emirates|    10,620,000|     20250.0|     24|            2|           [RB, CAM]|\n|nationality|               Egypt|   191,520,000|   18433.333|     26|           45|[RW, RM, RW, RM, ...|\n|nationality|  Dominican Republic|    10,030,000|     13800.0|     23|           10|[RW, RM, RW, GK, ...|\n+-----------+--------------------+--------------+------------+-------+-------------+--------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Wy\u015bwietlenie 50 pierwszych rekord\u00f3w\nfifa_players.show(50)"}, {"cell_type": "code", "execution_count": 36, "id": "5ff7fbed-f655-4f4b-9b54-7c03cabd3629", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"numTasks\": 105,\n  \"numActiveTasks\": 0,\n  \"numCompleteTasks\": 105,\n  \"numFailedTasks\": 0,\n  \"numKilledTasks\": 0,\n  \"numCompletedIndices\": 105,\n  \"executorDeserializeTime\": 965,\n  \"executorDeserializeCpuTime\": 788224290,\n  \"executorRunTime\": 2769,\n  \"executorCpuTime\": 2132498495,\n  \"resultSize\": 350918,\n  \"jvmGcTime\": 159,\n  \"resultSerializationTime\": 75,\n  \"memoryBytesSpilled\": 0,\n  \"diskBytesSpilled\": 0,\n  \"peakExecutionMemory\": 0,\n  \"inputBytes\": 34565313,\n  \"inputRecords\": 56939,\n  \"outputBytes\": 0,\n  \"outputRecords\": 0,\n  \"shuffleRemoteBlocksFetched\": 0,\n  \"shuffleLocalBlocksFetched\": 0,\n  \"shuffleFetchWaitTime\": 0,\n  \"shuffleRemoteBytesRead\": 0,\n  \"shuffleRemoteBytesReadToDisk\": 0,\n  \"shuffleLocalBytesRead\": 0,\n  \"shuffleReadBytes\": 0,\n  \"shuffleReadRecords\": 0,\n  \"shuffleWriteBytes\": 0,\n  \"shuffleWriteTime\": 0,\n  \"shuffleWriteRecords\": 0\n}\n"}], "source": "subtract_metrics(after_df_metrics, before_df_metrics)"}, {"cell_type": "code", "execution_count": null, "id": "7284bea2-d95c-4483-b0ce-d0ff1985e736", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "549891a9-7098-4c0e-879c-0ffc886bdacd", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "aefab783-50d5-4d00-936c-f440c697e3c6", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}